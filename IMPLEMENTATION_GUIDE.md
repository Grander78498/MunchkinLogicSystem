# –†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–π —Ä–∞–±–æ—Ç—ã ‚Ññ1

## –ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ

–î–∞–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–æ—à–∞–≥–æ–≤–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—é —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–π —Ä–∞–±–æ—Ç—ã ‚Ññ1 –ü–°–ü–ü–†.

## –¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞

### ‚úÖ –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ (70% —Ä–∞–±–æ—Ç—ã)

1. **–ú–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö** ([`models.py`](models.py:1-310))
   - –í—Å–µ –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏: –∫–æ–Ω—ä—é–Ω–∫—Ü–∏—è, –¥–∏–∑—ä—é–Ω–∫—Ü–∏—è, –∏–º–ø–ª–∏–∫–∞—Ü–∏—è, —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ü–∏—è, –æ—Ç—Ä–∏—Ü–∞–Ω–∏–µ
   - –ü—Ä–µ–¥–∏–∫–∞—Ç—ã –∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
   - –ö–ù–§ –∏ –¥–∏–∑—ä—é–Ω–∫—Ç—ã

2. **–õ–æ–≥–∏—á–µ—Å–∫–∏–π –¥–≤–∏–∂–æ–∫** ([`engine.py`](engine.py:1-269))
   - –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –ö–ù–§
   - –í—Å–µ –∑–∞–∫–æ–Ω—ã –ë—É–ª–µ–≤–æ–π –∞–ª–≥–µ–±—Ä—ã
   - –ú–µ—Ç–æ–¥ —Ä–µ–∑–æ–ª—é—Ü–∏–π (–ø—Ä–∞–≤–∏–ª–æ –≤—ã–≤–æ–¥–∞ P)
   - –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞–∫—Å–∏–æ–º

3. **–ë–∞–∑–æ–≤—ã–π –ª–µ–∫—Å–µ—Ä** ([`lexer.py`](lexer.py:1-72))
   - –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤
   - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Å—Ç—Ä–æ–∫ –≤ –∫–∞–≤—ã—á–∫–∞—Ö

### ‚ùå –¢—Ä–µ–±—É–µ—Ç—Å—è —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å (30% —Ä–∞–±–æ—Ç—ã)

1. **–ü–∞—Ä—Å–µ—Ä** - –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –≤ AST
2. **–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π** - —Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—ã—Å–∫–∞–∑—ã–≤–∞–Ω–∏–π –∏ –∞–∫—Å–∏–æ–º
3. **REPL-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å** - –∫–æ–Ω—Å–æ–ª—å–Ω–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ
4. **–ö–æ–º–∞–Ω–¥—ã** - help, get, load, remove
5. **–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫** - –ø–æ–Ω—è—Ç–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
6. **–ü—Ä–∏–º–µ—Ä—ã** - 3 –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ —Å–∏—Ç—É–∞—Ü–∏–∏

## –ü–æ—Ä—è–¥–æ–∫ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏

### –≠—Ç–∞–ø 1: –î–æ—Ä–∞–±–æ—Ç–∫–∞ –ª–µ–∫—Å–µ—Ä–∞ (2-3 —á–∞—Å–∞)

**–§–∞–π–ª**: [`lexer.py`](lexer.py)

**–ß—Ç–æ –¥–æ–±–∞–≤–∏—Ç—å**:

```python
class TokenType(str, Enum):
    IDENTIFIER = "identifier"
    EOL = ""
    
    # –õ–æ–≥–∏—á–µ—Å–∫–∏–µ –æ–ø–µ—Ä–∞—Ç–æ—Ä—ã
    OR = "OR"           # | –∏–ª–∏ +
    AND = "AND"         # & –∏–ª–∏ *
    NOT = "NOT"         # !
    IMPLIES = "IMPLIES" # ->
    EQUIVALENCE = "EQUALS"  # <->
    
    # –°–∫–æ–±–∫–∏
    LPAREN = "LPAREN"   # (
    RPAREN = "RPAREN"   # )
    
    # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ
    QUESTION = "QUESTION"  # ?
```

**–û–±–Ω–æ–≤–∏—Ç—å –º–µ—Ç–æ–¥ `tokenize_line()`**:

```python
def tokenize_line(self, text: str):
    self.current_pos = 0
    self.text = text
    self.current_char = self.text[0] if text else None
    self.tokens = []
    
    while self.current_char is not None:
        self.token_pos = self.current_pos
        
        # –ü—Ä–æ–ø—É—Å–∫ –ø—Ä–æ–±–µ–ª–æ–≤
        if self.current_char.isspace():
            self.advance()
            continue
        
        # –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
        if self.current_char == '/' and self.peek() == '/':
            break  # –û—Å—Ç–∞—Ç–æ–∫ —Å—Ç—Ä–æ–∫–∏ - –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π
        
        if self.current_char == '/' and self.peek() == '*':
            self.skip_multiline_comment()
            continue
        
        # –û–ø–µ—Ä–∞—Ç–æ—Ä—ã –∏–∑ –¥–≤—É—Ö —Å–∏–º–≤–æ–ª–æ–≤
        if self.current_char == '-' and self.peek() == '>':
            self.tokens.append(Token(TokenType.IMPLIES, '->', self.token_pos))
            self.advance(2)
            continue
        
        if self.current_char == '<' and self.peek() == '-' and self.peek(2) == '>':
            self.tokens.append(Token(TokenType.EQUIVALENCE, '<->', self.token_pos))
            self.advance(3)
            continue
        
        # –û–¥–∏–Ω–æ—á–Ω—ã–µ –æ–ø–µ—Ä–∞—Ç–æ—Ä—ã
        if self.current_char in '&*':
            self.tokens.append(Token(TokenType.AND, self.current_char, self.token_pos))
            self.advance()
            continue
        
        if self.current_char in '|+':
            self.tokens.append(Token(TokenType.OR, self.current_char, self.token_pos))
            self.advance()
            continue
        
        if self.current_char == '!':
            self.tokens.append(Token(TokenType.NOT, '!', self.token_pos))
            self.advance()
            continue
        
        if self.current_char == '(':
            self.tokens.append(Token(TokenType.LPAREN, '(', self.token_pos))
            self.advance()
            continue
        
        if self.current_char == ')':
            self.tokens.append(Token(TokenType.RPAREN, ')', self.token_pos))
            self.advance()
            continue
        
        if self.current_char == '?':
            self.tokens.append(Token(TokenType.QUESTION, '?', self.token_pos))
            self.advance()
            continue
        
        # –°—Ç—Ä–æ–∫–∏ –≤ –∫–∞–≤—ã—á–∫–∞—Ö
        if self.current_char == '"':
            self.advance()
            self.parse_identifier(skip_spaces=True)
            if self.current_char != '"':
                raise LexerException("–ù–µ–∑–∞–∫—Ä—ã—Ç–∞—è —Å—Ç—Ä–æ–∫–∞", self.token_pos)
            self.advance()
            continue
        
        # –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã
        if self.current_char.isalpha() or self.current_char == '_':
            self.parse_identifier()
            continue
        
        raise LexerException(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Å–∏–º–≤–æ–ª: {self.current_char}", self.token_pos)
    
    self.tokens.append(Token(TokenType.EOL, None, self.current_pos))
    return self.tokens

def peek(self, offset=1):
    """–ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π —Å–∏–º–≤–æ–ª –±–µ–∑ –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏—è"""
    pos = self.current_pos + offset
    if pos < len(self.text):
        return self.text[pos]
    return None

def skip_multiline_comment(self):
    """–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π"""
    self.advance(2)  # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å /*
    while self.current_char is not None:
        if self.current_char == '*' and self.peek() == '/':
            self.advance(2)
            break
        self.advance()
```

**–¢–µ—Å—Ç—ã** ([`tests/test_lexer.py`](tests/test_lexer.py)):

```python
TEST_DATA = [
    {
        "input": "a & b -> c",
        "expected": [
            Token(TokenType.IDENTIFIER, "a", 0),
            Token(TokenType.AND, "&", 2),
            Token(TokenType.IDENTIFIER, "b", 4),
            Token(TokenType.IMPLIES, "->", 6),
            Token(TokenType.IDENTIFIER, "c", 9),
            Token(TokenType.EOL, None, 10),
        ],
    },
    {
        "input": "!x | (y & z)",
        "expected": [
            Token(TokenType.NOT, "!", 0),
            Token(TokenType.IDENTIFIER, "x", 1),
            Token(TokenType.OR, "|", 3),
            Token(TokenType.LPAREN, "(", 5),
            Token(TokenType.IDENTIFIER, "y", 6),
            Token(TokenType.AND, "&", 8),
            Token(TokenType.IDENTIFIER, "z", 10),
            Token(TokenType.RPAREN, ")", 11),
            Token(TokenType.EOL, None, 12),
        ],
    },
]
```

### –≠—Ç–∞–ø 2: –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞—Ä—Å–µ—Ä–∞ (4-5 —á–∞—Å–æ–≤)

**–ù–æ–≤—ã–π —Ñ–∞–π–ª**: `parser.py`

**–ü–æ–ª–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è**:

```python
from typing import Optional
from lexer import Token, TokenType
from models import (
    Operation, Predicate, Variable,
    Conjunction, Disjunction, Negation,
    Implication, Equivalence
)


class ParserException(Exception):
    """–ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø–∞—Ä—Å–µ—Ä–∞"""
    def __init__(self, message: str, token: Optional[Token] = None):
        self.token = token
        if token:
            super().__init__(
                f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–∞ –ø–æ–∑–∏—Ü–∏–∏ {token.position}: {message}"
            )
        else:
            super().__init__(message)


class Parser:
    """–°–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π"""
    
    def __init__(self, tokens: list[Token]):
        self.tokens = tokens
        self.current = 0
    
    def parse(self) -> Operation | Predicate:
        """–ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        if not self.tokens or self.tokens[0].type == TokenType.EOL:
            raise ParserException("–ü—É—Å—Ç–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ")
        
        result = self.parse_equivalence()
        
        if not self.is_at_end():
            raise ParserException(
                f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω: {self.peek().value}",
                self.peek()
            )
        
        return result
    
    def parse_equivalence(self) -> Operation | Predicate:
        """–ü–∞—Ä—Å–∏–Ω–≥ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ü–∏–∏ (—Å–∞–º—ã–π –Ω–∏–∑–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)"""
        left = self.parse_implication()
        
        if self.match(TokenType.EQUIVALENCE):
            right = self.parse_implication()
            return Equivalence((left, right))
        
        return left
    
    def parse_implication(self) -> Operation | Predicate:
        """–ü–∞—Ä—Å–∏–Ω–≥ –∏–º–ø–ª–∏–∫–∞—Ü–∏–∏"""
        left = self.parse_disjunction()
        
        if self.match(TokenType.IMPLIES):
            right = self.parse_disjunction()
            return Implication((left, right))
        
        return left
    
    def parse_disjunction(self) -> Operation | Predicate:
        """–ü–∞—Ä—Å–∏–Ω–≥ –¥–∏–∑—ä—é–Ω–∫—Ü–∏–∏"""
        left = self.parse_conjunction()
        
        while self.match(TokenType.OR):
            right = self.parse_conjunction()
            left = Disjunction((left, right))
        
        return left
    
    def parse_conjunction(self) -> Operation | Predicate:
        """–ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–Ω—ä—é–Ω–∫—Ü–∏–∏"""
        left = self.parse_negation()
        
        while self.match(TokenType.AND):
            right = self.parse_negation()
            left = Conjunction((left, right))
        
        return left
    
    def parse_negation(self) -> Operation | Predicate:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç—Ä–∏—Ü–∞–Ω–∏—è"""
        if self.match(TokenType.NOT):
            operand = self.parse_negation()
            return Negation(operand)
        
        return self.parse_primary()
    
    def parse_primary(self) -> Operation | Predicate:
        """–ü–∞—Ä—Å–∏–Ω–≥ –±–∞–∑–æ–≤—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤"""
        # –°–∫–æ–±–∫–∏
        if self.match(TokenType.LPAREN):
            expr = self.parse_equivalence()
            if not self.match(TokenType.RPAREN):
                raise ParserException(
                    "–û–∂–∏–¥–∞–ª–∞—Å—å –∑–∞–∫—Ä—ã–≤–∞—é—â–∞—è —Å–∫–æ–±–∫–∞",
                    self.peek()
                )
            return expr
        
        # –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä
        if self.match(TokenType.IDENTIFIER):
            name = self.previous().value
            return Variable(name)
        
        raise ParserException(
            f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω: {self.peek().value if not self.is_at_end() else '–∫–æ–Ω–µ—Ü –≤—ã—Ä–∞–∂–µ–Ω–∏—è'}",
            self.peek() if not self.is_at_end() else None
        )
    
    # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã
    
    def match(self, *types: TokenType) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∏ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—å—Å—è, –µ—Å–ª–∏ —Ç–µ–∫—É—â–∏–π —Ç–æ–∫–µ–Ω —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ç–∏–ø—É"""
        for token_type in types:
            if self.check(token_type):
                self.advance()
                return True
        return False
    
    def check(self, token_type: TokenType) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ç–∏–ø —Ç–µ–∫—É—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞"""
        if self.is_at_end():
            return False
        return self.peek().type == token_type
    
    def advance(self) -> Token:
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—å—Å—è –∫ —Å–ª–µ–¥—É—é—â–µ–º—É —Ç–æ–∫–µ–Ω—É"""
        if not self.is_at_end():
            self.current += 1
        return self.previous()
    
    def is_at_end(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –¥–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏ –∫–æ–Ω–µ—Ü"""
        return self.peek().type == TokenType.EOL
    
    def peek(self) -> Token:
        """–ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å —Ç–µ–∫—É—â–∏–π —Ç–æ–∫–µ–Ω"""
        return self.tokens[self.current]
    
    def previous(self) -> Token:
        """–ü–æ–ª—É—á–∏—Ç—å –ø—Ä–µ–¥—ã–¥—É—â–∏–π —Ç–æ–∫–µ–Ω"""
        return self.tokens[self.current - 1]
```

**–¢–µ—Å—Ç—ã** (`tests/test_parser.py`):

```python
import pytest
from lexer import Lexer
from parser import Parser, ParserException
from models import *


def parse_expression(text: str):
    """–í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞"""
    lexer = Lexer()
    tokens = lexer.tokenize_line(text)
    parser = Parser(tokens)
    return parser.parse()


TEST_DATA = [
    ("a", Variable("a")),
    ("a & b", Conjunction((Variable("a"), Variable("b")))),
    ("a | b", Disjunction((Variable("a"), Variable("b")))),
    ("!a", Negation(Variable("a"))),
    ("a -> b", Implication((Variable("a"), Variable("b")))),
    ("a <-> b", Equivalence((Variable("a"), Variable("b")))),
    ("(a & b) | c", Disjunction((Conjunction((Variable("a"), Variable("b"))), Variable("c")))),
    ("a & (b | c)", Conjunction((Variable("a"), Disjunction((Variable("b"), Variable("c")))))),
]


@pytest.mark.parametrize(("input", "expected"), TEST_DATA)
def test_parser(input: str, expected):
    result = parse_expression(input)
    assert str(result) == str(expected)
```

### –≠—Ç–∞–ø 3: –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π (2-3 —á–∞—Å–∞)

**–ù–æ–≤—ã–π —Ñ–∞–π–ª**: `knowledge_base.py`

```python
from dataclasses import dataclass
from typing import Optional
from models import Operation, Predicate


@dataclass
class Statement:
    """–í—ã—Å–∫–∞–∑—ã–≤–∞–Ω–∏–µ –≤ –∞–ª—Ñ–∞–≤–∏—Ç–µ"""
    id: int
    name: str
    description: Optional[str] = None
    
    def __str__(self):
        return f"[{self.id}] {self.name}"


@dataclass
class Axiom:
    """–ê–∫—Å–∏–æ–º–∞ (–¥–∏–∑—ä—é–Ω–∫—Ç –•–æ—Ä–Ω–∞)"""
    id: int
    expression: Operation
    description: Optional[str] = None
    
    def __str__(self):
        return f"({self.id}) {self.expression}"


class KnowledgeBase:
    """–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π - —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –≤—ã—Å–∫–∞–∑—ã–≤–∞–Ω–∏–π –∏ –∞–∫—Å–∏–æ–º"""
    
    def __init__(self):
        self.statements: dict[str, Statement] = {}
        self.axioms: list[Axiom] = []
        self._next_statement_id = 1
        self._next_axiom_id = 1
    
    def add_statement(self, name: str, description: str = None) -> Statement:
        """–î–æ–±–∞–≤–∏—Ç—å –≤—ã—Å–∫–∞–∑—ã–≤–∞–Ω–∏–µ –≤ –∞–ª—Ñ–∞–≤–∏—Ç"""
        if name in self.statements:
            return self.statements[name]
        
        statement = Statement(
            id=self._next_statement_id,
            name=name,
            description=description
        )
        self.statements[name] = statement
        self._next_statement_id += 1
        return statement
    
    def add_axiom(self, expression: Operation, description: str = None) -> Axiom:
        """–î–æ–±–∞–≤–∏—Ç—å –∞–∫—Å–∏–æ–º—É"""
        axiom = Axiom(
            id=self._next_axiom_id,
            expression=expression,
            description=description
        )
        self.axioms.append(axiom)
        self._next_axiom_id += 1
        return axiom
    
    def remove_axiom(self, axiom_id: int) -> bool:
        """–£–¥–∞–ª–∏—Ç—å –∞–∫—Å–∏–æ–º—É –ø–æ ID"""
        for i, axiom in enumerate(self.axioms):
            if axiom.id == axiom_id:
                self.axioms.pop(i)
                return True
        return False
    
    def get_statement(self, name: str) -> Optional[Statement]:
        """–ü–æ–ª—É—á–∏—Ç—å –≤—ã—Å–∫–∞–∑—ã–≤–∞–Ω–∏–µ –ø–æ –∏–º–µ–Ω–∏"""
        return self.statements.get(name)
    
    def get_axiom(self, axiom_id: int) -> Optional[Axiom]:
        """–ü–æ–ª—É—á–∏—Ç—å –∞–∫—Å–∏–æ–º—É –ø–æ ID"""
        for axiom in self.axioms:
            if axiom.id == axiom_id:
                return axiom
        return None
    
    def get_all_statements(self) -> list[Statement]:
        """–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ –≤—ã—Å–∫–∞–∑—ã–≤–∞–Ω–∏—è"""
        return sorted(self.statements.values(), key=lambda s: s.id)
    
    def get_all_axioms(self) -> list[Axiom]:
        """–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ –∞–∫—Å–∏–æ–º—ã"""
        return self.axioms.copy()
    
    def clear(self):
        """–û—á–∏—Å—Ç–∏—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"""
        self.statements.clear()
        self.axioms.clear()
        self._next_statement_id = 1
        self._next_axiom_id = 1
    
    def __str__(self):
        result = ["=== –ë–ê–ó–ê –ó–ù–ê–ù–ò–ô ==="]
        result.append(f"\n–í—ã—Å–∫–∞–∑—ã–≤–∞–Ω–∏—è ({len(self.statements)}):")
        for stmt in self.get_all_statements():
            result.append(f"  {stmt}")
        result.append(f"\n–ê–∫—Å–∏–æ–º—ã ({len(self.axioms)}):")
        for axiom in self.axioms:
            result.append(f"  {axiom}")
        return "\n".join(result)
```

### –≠—Ç–∞–ø 4: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –¥–≤–∏–∂–∫–æ–º (1 —á–∞—Å)

**–û–±–Ω–æ–≤–∏—Ç—å** [`engine.py`](engine.py):

```python
# –í –Ω–∞—á–∞–ª–µ —Ñ–∞–π–ª–∞ –¥–æ–±–∞–≤–∏—Ç—å –∏–º–ø–æ—Ä—Ç
from knowledge_base import KnowledgeBase

# –û–±–Ω–æ–≤–∏—Ç—å –∫–ª–∞—Å—Å LogicalEngine
class LogicalEngine:
    def __init__(self, knowledge_base: KnowledgeBase = None):
        self.kb = knowledge_base or KnowledgeBase()
        self.axioms: list[Disjunct] = []
    
    def load_axioms_from_kb(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∞–∫—Å–∏–æ–º—ã –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –≤ –¥–≤–∏–∂–æ–∫"""
        self.axioms.clear()
        for axiom in self.kb.get_all_axioms():
            cnf = self.to_cnf(axiom.expression, output=False)
            if cnf.children:
                self.axioms.extend(cnf.children)
```

### –≠—Ç–∞–ø 5: REPL-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å (5-6 —á–∞—Å–æ–≤)

**–ù–æ–≤—ã–π —Ñ–∞–π–ª**: `repl.py`

–°–º. –ø–æ–¥—Ä–æ–±–Ω—É—é —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é –≤ [`PLAN.md`](PLAN.md) (–§–∞–∑–∞ 3, —Ä–∞–∑–¥–µ–ª 3.1-3.3)

–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:
- –ö–ª–∞—Å—Å `REPL` —Å –º–µ—Ç–æ–¥–æ–º `run()`
- –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–º–∞–Ω–¥: help, get, load, remove, clear, exit
- –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã—Å–∫–∞–∑—ã–≤–∞–Ω–∏–π, –∞–∫—Å–∏–æ–º –∏ —Ç–µ–æ—Ä–µ–º
- –ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Unicode-—Å–∏–º–≤–æ–ª–æ–≤

### –≠—Ç–∞–ø 6: –ü—Ä–∏–º–µ—Ä—ã –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è (2-3 —á–∞—Å–∞)

**–°–æ–∑–¥–∞—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é** `examples/` —Å —Ñ–∞–π–ª–∞–º–∏:
- `situation1.shldn` - –Ω–æ—Ä–º–∞–ª—å–Ω–∞—è —Å–∏—Ç—É–∞—Ü–∏—è
- `situation2.shldn` - —Ü–µ–ª–µ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
- `situation3.shldn` - –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è

**–°–æ–∑–¥–∞—Ç—å** `demo.py` –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏

–°–º. –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ –≤ [`PLAN.md`](PLAN.md) (–§–∞–∑–∞ 5)

### –≠—Ç–∞–ø 7: –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è (1-2 —á–∞—Å–∞)

**–û–±–Ω–æ–≤–∏—Ç—å** [`README.md`](README.md):
- –û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞
- –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –∑–∞–ø—É—Å–∫
- –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
- –°–∏–Ω—Ç–∞–∫—Å–∏—Å —è–∑—ã–∫–∞
- –ö–æ–º–∞–Ω–¥—ã REPL

## –û—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏

| –≠—Ç–∞–ø | –í—Ä–µ–º—è | –°–ª–æ–∂–Ω–æ—Å—Ç—å |
|------|-------|-----------|
| 1. –î–æ—Ä–∞–±–æ—Ç–∫–∞ –ª–µ–∫—Å–µ—Ä–∞ | 2-3 —á | –°—Ä–µ–¥–Ω—è—è |
| 2. –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞—Ä—Å–µ—Ä–∞ | 4-5 —á | –í—ã—Å–æ–∫–∞—è |
| 3. –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π | 2-3 —á | –ù–∏–∑–∫–∞—è |
| 4. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –¥–≤–∏–∂–∫–æ–º | 1 —á | –ù–∏–∑–∫–∞—è |
| 5. REPL-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å | 5-6 —á | –°—Ä–µ–¥–Ω—è—è |
| 6. –ü—Ä–∏–º–µ—Ä—ã –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è | 2-3 —á | –ù–∏–∑–∫–∞—è |
| 7. –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è | 1-2 —á | –ù–∏–∑–∫–∞—è |
| **–ò–¢–û–ì–û** | **17-23 —á** | - |

## –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏

### 1. –ü–æ—Ä—è–¥–æ–∫ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏

–°–ª–µ–¥—É–π—Ç–µ —É–∫–∞–∑–∞–Ω–Ω–æ–º—É –ø–æ—Ä—è–¥–∫—É —ç—Ç–∞–ø–æ–≤ - –∫–∞–∂–¥—ã–π —Å–ª–µ–¥—É—é—â–∏–π –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ.

### 2. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

–ü–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —ç—Ç–∞–ø–∞ –ø–∏—à–∏—Ç–µ –∏ –∑–∞–ø—É—Å–∫–∞–π—Ç–µ —Ç–µ—Å—Ç—ã:
```bash
pytest tests/test_lexer.py
pytest tests/test_parser.py
pytest tests/test_kb.py
```

### 3. –û—Ç–ª–∞–¥–∫–∞

–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –æ—Ç–ª–∞–¥–æ—á–Ω—ã–π –≤—ã–≤–æ–¥:
```python
# –í –ø–∞—Ä—Å–µ—Ä–µ
def parse(self):
    print(f"DEBUG: Parsing tokens: {self.tokens}")
    result = self.parse_equivalence()
    print(f"DEBUG: Result: {result}")
    return result
```

### 4. –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞

–ù–µ –ø—ã—Ç–∞–π—Ç–µ—Å—å —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –≤—Å—ë —Å—Ä–∞–∑—É. –ù–∞—á–Ω–∏—Ç–µ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏:
1. –ü—Ä–æ—Å—Ç–æ–π –ª–µ–∫—Å–µ—Ä (—Ç–æ–ª—å–∫–æ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –∏ –æ–¥–∏–Ω –æ–ø–µ—Ä–∞—Ç–æ—Ä)
2. –ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–µ—Ä (—Ç–æ–ª—å–∫–æ –∫–æ–Ω—ä—é–Ω–∫—Ü–∏—è)
3. –ë–∞–∑–æ–≤—ã–π REPL (—Ç–æ–ª—å–∫–æ –∫–æ–º–∞–Ω–¥–∞ help)
4. –ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ –¥–æ–±–∞–≤–ª—è–π—Ç–µ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å

### 5. –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫

–î–æ–±–∞–≤–ª—è–π—Ç–µ –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–æ–∫ —Å—Ä–∞–∑—É, –Ω–µ –æ—Ç–∫–ª–∞–¥—ã–≤–∞–π—Ç–µ –Ω–∞ –ø–æ—Ç–æ–º:
```python
try:
    result = parser.parse()
except ParserException as e:
    print(f"–û—à–∏–±–∫–∞: {e}")
    return
```

## –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º

### –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∏–∑ –∑–∞–¥–∞–Ω–∏—è

- [x] **–ó–∞–¥–∞—á–∞ 1**: –°–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π —è–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è ‚úì
  - –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω –∫–∞–∫ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –Ω–∞ Python
  - –ö–æ–Ω—Å–æ–ª—å–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ (REPL)
  - –õ–µ–∫—Å–µ—Ä –∏ –ø–∞—Ä—Å–µ—Ä
  - –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∏—Å–∫–ª—é—á–µ–Ω–∏–π

- [x] **–ó–∞–¥–∞—á–∞ 2**: –û–ø–µ—Ä–∞—Ü–∏–∏ –∏ —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –∞–ª–≥–µ–±—Ä—ã –ª–æ–≥–∏–∫–∏ ‚úì
  - –ö–æ–Ω—ä—é–Ω–∫—Ü–∏—è, –¥–∏–∑—ä—é–Ω–∫—Ü–∏—è, –∏–º–ø–ª–∏–∫–∞—Ü–∏—è, —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ü–∏—è, –æ—Ç—Ä–∏—Ü–∞–Ω–∏–µ
  - –ó–∞–∫–æ–Ω—ã –ë—É–ª–µ–≤–æ–π –∞–ª–≥–µ–±—Ä—ã —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω—ã

- [x] **–ó–∞–¥–∞—á–∞ 3**: –ë–∞–∑–∞ –ó–Ω–∞–Ω–∏–π ‚úì
  - –ê–ª—Ñ–∞–≤–∏—Ç (–≤—ã—Å–∫–∞–∑—ã–≤–∞–Ω–∏—è)
  - –ê–∫—Å–∏–æ–º—ã (–¥–∏–∑—ä—é–Ω–∫—Ç—ã –•–æ—Ä–Ω–∞)

- [x] **–ó–∞–¥–∞—á–∞ 4**: –ü—Ä–∞–≤–∏–ª–∞ –≤—ã–≤–æ–¥–∞ ‚úì
  - –ú–µ—Ç–æ–¥ —Ä–µ–∑–æ–ª—é—Ü–∏–π —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω

- [x] **–ó–∞–¥–∞—á–∞ 5**: –ü—Ä–µ–¥–º–µ—Ç–Ω–∞—è –æ–±–ª–∞—Å—Ç—å ‚úì
  - –ò–≥—Ä–∞ –ú–∞–Ω—á–∫–∏–Ω (–∫–∞—Ä—Ç–æ—á–Ω–∞—è –∏–≥—Ä–∞)

- [x] **–ó–∞–¥–∞—á–∞ 6**: –ü—Ä–æ–±–ª–µ–º–Ω—ã–µ —Å–∏—Ç—É–∞—Ü–∏–∏ ‚úì
  - 3 —Å–∏—Ç—É–∞—Ü–∏–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã

### –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è

- [x] –§—É–Ω–∫—Ü–∏—è `help` ‚úì
- [x] –í—ã–≤–æ–¥ –∞–ª—Ñ–∞–≤–∏—Ç–∞ –∏ –∞–∫—Å–∏–æ–º (`get`) ‚úì
- [x] –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã ‚úì
- [x] –°–∏–Ω—Ç–∞–∫—Å–∏—Å –¥–ª—è –≤–≤–æ–¥–∞ –¥–∞–Ω–Ω—ã—Ö ‚úì

## –ó–∞–ø—É—Å–∫ –≥–æ—Ç–æ–≤–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞

–ü–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏:

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
pip install -r requirements.txt

# –ó–∞–ø—É—Å–∫ REPL
python main.py

# –ó–∞–ø—É—Å–∫ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
python demo.py

# –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤
pytest
```

## –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞

```
MunchkinLogicSystem/
‚îú‚îÄ‚îÄ models.py              # ‚úÖ –ì–æ—Ç–æ–≤–æ
‚îú‚îÄ‚îÄ engine.py              # ‚úÖ –ì–æ—Ç–æ–≤–æ (—Ç—Ä–µ–±—É–µ—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π)
‚îú‚îÄ‚îÄ lexer.py               # ‚ö†Ô∏è –¢—Ä–µ–±—É–µ—Ç –¥–æ—Ä–∞–±–æ—Ç–∫–∏
‚îú‚îÄ‚îÄ parser.py              # ‚ùå –°–æ–∑–¥–∞—Ç—å
‚îú‚îÄ‚îÄ knowledge_base.py      # ‚ùå –°–æ–∑–¥–∞—Ç—å
‚îú‚îÄ‚îÄ repl.py                # ‚ùå –°–æ–∑–¥–∞—Ç—å
‚îú‚îÄ‚îÄ main.py                # ‚ö†Ô∏è –û–±–Ω–æ–≤–∏—Ç—å (—Ç–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –≤ REPL)
‚îú‚îÄ‚îÄ demo.py                # ‚ùå –°–æ–∑–¥–∞—Ç—å
‚îú‚îÄ‚îÄ README.md              # ‚ö†Ô∏è –û–±–Ω–æ–≤–∏—Ç—å
‚îú‚îÄ‚îÄ PLAN.md                # ‚úÖ –ì–æ—Ç–æ–≤–æ
‚îú‚îÄ‚îÄ ARCHITECTURE.md        # ‚úÖ –ì–æ—Ç–æ–≤–æ
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_lexer.py      # ‚ö†Ô∏è –î–æ–ø–æ–ª–Ω–∏—Ç—å
‚îÇ   ‚îú‚îÄ‚îÄ test_parser.py     # ‚ùå –°–æ–∑–¥–∞—Ç—å
‚îÇ   ‚îú‚îÄ‚îÄ test_engine.py     # ‚úÖ –ì–æ—Ç–æ–≤–æ
‚îÇ   ‚îî‚îÄ‚îÄ test_kb.py         # ‚ùå –°–æ–∑–¥–∞—Ç—å
‚îî‚îÄ‚îÄ examples/
    ‚îú‚îÄ‚îÄ situation1.shldn   # ‚ùå –°–æ–∑–¥–∞—Ç—å
    ‚îú‚îÄ‚îÄ situation2.shldn   # ‚ùå –°–æ–∑–¥–∞—Ç—å
    ‚îî‚îÄ‚îÄ situation3.shldn   # ‚ùå –°–æ–∑–¥–∞—Ç—å
```

## –ü–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏

- [–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è Python](https://docs.python.org/3/)
- [Pytest –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è](https://docs.pytest.org/)
- [–ò—Å—á–∏—Å–ª–µ–Ω–∏–µ –≤—ã—Å–∫–∞–∑—ã–≤–∞–Ω–∏–π](https://ru.wikipedia.org/wiki/–ò—Å—á–∏—Å–ª–µ–Ω–∏–µ_–≤—ã—Å–∫–∞–∑—ã–≤–∞–Ω–∏–π)
- [–ú–µ—Ç–æ–¥ —Ä–µ–∑–æ–ª—é—Ü–∏–π](https://ru.wikipedia.org/wiki/–ú–µ—Ç–æ–¥_—Ä–µ–∑–æ–ª—é—Ü–∏–π)
- [–ö–ù–§](https://ru.wikipedia.org/wiki/–ö–æ–Ω—ä—é–Ω–∫—Ç–∏–≤–Ω–∞—è_–Ω–æ—Ä–º–∞–ª—å–Ω–∞—è_—Ñ–æ—Ä–º–∞)

## –ö–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ –ø–µ—Ä–µ–¥ —Å–¥–∞—á–µ–π

- [ ] –í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ—Ö–æ–¥—è—Ç
- [ ] REPL –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç
- [ ] –ö–æ–º–∞–Ω–¥–∞ `help` –≤—ã–≤–æ–¥–∏—Ç —Å–ø—Ä–∞–≤–∫—É
- [ ] –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–ª—è—Ç—å –≤—ã—Å–∫–∞–∑—ã–≤–∞–Ω–∏—è –∏ –∞–∫—Å–∏–æ–º—ã
- [ ] –ú–æ–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä—è—Ç—å —Ç–µ–æ—Ä–µ–º—ã
- [ ] –ö–æ–º–∞–Ω–¥–∞ `load` –∑–∞–≥—Ä—É–∂–∞–µ—Ç —Ñ–∞–π–ª—ã
- [ ] 3 –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ —Å–∏—Ç—É–∞—Ü–∏–∏ —Ä–∞–±–æ—Ç–∞—é—Ç
- [ ] –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–∞
- [ ] –ö–æ–¥ –ø—Ä–æ–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω
- [ ] –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç

## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

–°–ª–µ–¥—É—è —ç—Ç–æ–º—É —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤—É, –≤—ã —Å–º–æ–∂–µ—Ç–µ –∑–∞–≤–µ—Ä—à–∏—Ç—å —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–π —Ä–∞–±–æ—Ç—ã –∑–∞ 17-23 —á–∞—Å–∞ —á–∏—Å—Ç–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏. –ü—Ä–æ–µ–∫—Ç —É–∂–µ –Ω–∞ 70% –≥–æ—Ç–æ–≤, –æ—Å—Ç–∞–ª–æ—Å—å —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø–∞—Ä—Å–µ—Ä, –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å.

–£–¥–∞—á–∏ –≤ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏! üöÄ